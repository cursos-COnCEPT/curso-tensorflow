{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "- Aprender a cargar datos de un archivo CSV en un DataFrame de Pandas.\n",
    "- Comprender la importancia del preprocesamiento de datos en el entrenamiento de redes neuronales.\n",
    "- Crear nuestro primer modelo de una red neuronal.\n",
    "- Evaluar el modelo y realizar predicicones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos experimental: bioreactor\n",
    "El primer paso es cargar los datos desde el archivo `Experiment.csv` a un `DataFrame` utilizndo la librería **pandas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLC_start</th>\n",
       "      <th>feed_start</th>\n",
       "      <th>feed_end</th>\n",
       "      <th>feed_rate</th>\n",
       "      <th>VCD_start</th>\n",
       "      <th>VCD_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.552323</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10.609717</td>\n",
       "      <td>0.174075</td>\n",
       "      <td>1420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.214075</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7.046333</td>\n",
       "      <td>0.869415</td>\n",
       "      <td>654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.289962</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>16.556856</td>\n",
       "      <td>0.724800</td>\n",
       "      <td>1180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.727849</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>19.262477</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.845139</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.105958</td>\n",
       "      <td>0.969275</td>\n",
       "      <td>842.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLC_start  feed_start  feed_end  feed_rate  VCD_start  VCD_end\n",
       "0  64.552323           4        10  10.609717   0.174075   1420.0\n",
       "1  25.214075           2        11   7.046333   0.869415    654.0\n",
       "2  23.289962           3         8  16.556856   0.724800   1180.0\n",
       "3  25.727849           3        10  19.262477   0.349741   1440.0\n",
       "4  55.845139           2        10   5.105958   0.969275    842.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  dataset_path = 'https://raw.githubusercontent.com/cursos-COnCEPT/curso-tensorflow/refs/heads/main/Experiment.csv?token=GHSAT0AAAAAAC6J2VALSXZW354GPERHUJTSZ5S7PKQ'\n",
    "else:\n",
    "  dataset_path = os.getcwd() + '\\\\Experiment.csv'\n",
    "\n",
    "raw_dataset = pd.read_csv(dataset_path)\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunos casos, resulta interesante explorar los datos antendiendo a medidas estadísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como representar los datos de forma gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_feature in dataset.columns:\n",
    "    if input_feature != 'VCD_end':\n",
    "        dataset.plot.scatter(x=input_feature, y='VCD_end')\n",
    "        plt.xlabel(input_feature)\n",
    "        plt.ylabel('VCD_end')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "### 1 - División o *train-test-validation split*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro objetivo es crear un modelo que generalice bien a los datos no vistos y no simplemente que se ajuste perfectamente a algunos datos vistos. Por este motivo debemos asegurar que al dividir los datos, los conjunto de `test`sea lo más parecido a los datos futuros en los que se quiere aplicar el modelo. \n",
    "\n",
    "En este caso, basta con dividir nuestro `dataset` de forma aleatoria. Sin embargo, hay situaciones más complejas en las que hay que tener cuidado con cómo se hace el reparto. Puedes aprender más sobre el *stratified splitting* [aquí](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = .7\n",
    "test_ratio = .15\n",
    "val_ratio = .15\n",
    "\n",
    "X_train = dataset.sample(frac=train_ratio+val_ratio, random_state=0)\n",
    "X_test = dataset.drop(X_train.index)\n",
    "X_val = X_train.sample(frac=val_ratio/(val_ratio+train_ratio), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Separación inputs y outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train.pop('VCD_end')\n",
    "y_test = X_test.pop('VCD_end')\n",
    "y_val = X_val.pop('VCD_end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Escalado \n",
    "El análisis estadístico ha sido útil para identificar cómo las variables de entrada tienen diferentes escalas, lo cual puede afectar al proceso de entrenamiento, ya que las variables que se mueven en un rango más amplio tienen mayor impacto sobre el modelo.\n",
    "\n",
    "Para evitar que esto ocurra, es recomendable escalar los datos. Para este ejemplo, vamos a utilizar la función `MinMaxScaler` del paquete **sklearn**. Internamente, esta función realiza la siguiente operación:\n",
    "```python\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero inicializamos nuestra función de escalado y después llamamos al comando `fit` para obtener los parámetros de la función, esto es, los valores mínimo y máximo de cada variable de entrada en el training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atención:** el escalado no se aplica a los outputs. ([*Should you scale target values in a regression problem?*](https://www.kaggle.com/discussions/questions-and-answers/181908)).\n",
    "\n",
    "A continuación, escalamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nondim = scaler.transform(X_train)\n",
    "X_test_nondim = scaler.transform(X_test)\n",
    "X_val_nondim = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio - leer la documentación\n",
    "Para aprender sobre *machine learning* o programación en general, es fundamental saber manejarse con la documentación de las diferentes herramientas y extraer la información que nos interesa.\n",
    "\n",
    "Visita la [web de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) y contesta la siguiente pregunta:\n",
    "- ¿Cuáles son los valores de `X.min` y `X.max` que se utilizan ara escalar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa con tu código aquí\n",
    "# min_values = \n",
    "# max_values = \n",
    "\n",
    "# print('Los valores mínimos y máximos para cada variable son:')\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    print(f'{col}: {min_values[i]} - {max_values[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante conocer qué valores se han utilizado para hacer el escalado de los datos de entrenamiento porque luego deberán aplicarse los mismos a cualquier otro dato que se alimente al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Definición del modelo\n",
    "Aquí vamos a utilizar un modelo secuencial con 3 capas internas con 128, 64 y 32 neuronas, respectivamente, seguidas de una capa de salida que devuelve un único valor, el de la concentración de VCD al final del experimento. Para las capas internas, aplica funciones de activación tipo ReLU. En problemas de regresión, no suele aplicarse ninguna función de activación en la *output layer*.\n",
    "\n",
    "Aunque no es estrictamente necesario, vamos a agrupar todos los pasos de la construcción del modelo como una función (se llamará `build_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1]\n",
    "num_hidden_neurons = [128, 64, 32]\n",
    "num_outputs = 1\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 10\n",
    "N_TRAIN = X_train.shape[0]\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "\n",
    "initial_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de modelos convergen mejor cuando se disminuye el *learning rate* de forma gradual durante el entrenamiento. \n",
    "\n",
    "Para modular cómo varía el *learning rate* se utilizan funciones llamadas *learning rate schedules*. Existen diversos *schedules* ya implementados en TensorFloW, entre los que vamos a utilizar [InverseTimeDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=STEPS_PER_EPOCH*50,\n",
    "    decay_rate=1,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de arriba crea un `tf.keras.optimizers.schedules.InverseTimeDecay` para disminuir el *learning rate* a la mitad pasadas las primeras 50 épocas, 1/3 a las 100, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = np.linspace(0,EPOCHS*STEPS_PER_EPOCH)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_hidden_neurons, lr):\n",
    "    layers_list = []\n",
    "    layers_list.append(layers.InputLayer(shape=[len(X_train.columns)],\n",
    "                                         name = 'INPUT'))\n",
    "    for i in range(len(num_hidden_neurons)):\n",
    "        layers_list.append(layers.Dense(num_hidden_neurons[i], \n",
    "                                        activation='relu', \n",
    "                                        kernel_initializer='he_normal', \n",
    "                                        name = f'HIDDEN_{i}'))\n",
    "    layers_list.append(layers.Dense(num_outputs, \n",
    "                        activation = 'linear', \n",
    "                        name = 'OUTPUT'))\n",
    "\n",
    "    model = keras.Sequential(layers_list)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    model.compile(  loss='mse',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construido, el método `summary` permite inspeccionar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(num_hidden_neurons, lr_schedule)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teóricamente, ya podríamos utilizar el modelo para hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = X_train_nondim[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Real Values': y_train[:10].values,\n",
    "    'Predicted Values': example_result.flatten()\n",
    "})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Entrenamiento\n",
    "Aunque el modelo parece funcionar y da un resultado de la forma y tipo esperados, las predicciones no se ajustan a los valores esperados. Esto se debe a que todavía no está entrenado, es decir, hay que optimizar los valors de los pesos o `weights` de la red neuronal.\n",
    "\n",
    "Entrena el modelo durante 200 épocas y representa la curva de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_nondim, y_train,\n",
    "        epochs=EPOCHS, \n",
    "        batch_size = BATCH_SIZE,\n",
    "        validation_data = (X_val_nondim, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [UNITS]')\n",
    "  plt.plot(hist['epoch'], hist['mae'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mae'],\n",
    "           label = 'Val Error')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$UNITS^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mse'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mse'],\n",
    "           label = 'Validation Error')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(X_test_nondim, y_test, verbose=2)\n",
    "\n",
    "print(\"MAE en test: {:5.2f} UNITS\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Realizar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test_nondim).flatten()\n",
    "\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('Real Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "p1 = max(max(test_predictions), max(y_test))\n",
    "p2 = min(min(test_predictions), min(y_test))\n",
    "plt.plot([p1, p2], [p1, p2], '--k')\n",
    "plt.xlim([p1,p2])\n",
    "plt.ylim([p1,p2])\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos visualizar la distribución de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - y_test\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error [UNITS]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Con este ejemplo hemos aprendido:\n",
    "\n",
    "* El error cuadrático medio (MSE) es una *loss function* muy utilizada para problemas de regresión (se utilizan diferentes funciones de pérdida para problemas de clasificación).\n",
    "* Otra métrica muy común para evaluar el modelo es el error absoluto medio (MAE).\n",
    "* Cuando las *inputs* tienen valores con diferentes rangos, deben aplicarse técnicas de escalado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este modelo se va a aprender a crear una red neuronal\n",
    "con otro ejemplo se va a ver el overfitting + early stopping\n",
    "con otro diferente el hyperparam tunning\n",
    "se vuelve a este si da tiempo para el concurso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
